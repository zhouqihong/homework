{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总的来说，就是把一个信息压缩，然后再从压缩信息还原到原始信息的一个过程。\n",
    "\n",
    "autoencode先通过encoder把输入数据压缩成保留输入数据最大信息的低维数据，再通过decoder把latent space中的低维数据还原为和高维数据。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "greedy search贪心搜索：在RNN处理序列过程中，greedy search对于序列的每一个元素都取概率最大的结果往下个节点传递，最后得到的结果是所有元素的最大结果的拼接。只能保证每一步最优，不能保证最终结果最优\n",
    "\n",
    "beam search集束搜索：在RNN处理序列的过程中，beam search对于序列的每一元素都取概率前N大的结果往下个节点传递，由于每个节点都取前N大的结果，最后会形成一个树形结构，最后得到的结果需要从这个树形结构中找到权重最大的分支。最终在beam个结果中选取概率最大的结果，保证局部最优。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在普通的seq2seq模型中，encoder把所有输入编成一个context向量，即encoder最后一个隐藏层的状态，然后喂给decoder解码，但是对于一个长句子来说一个context向量可能没法保留整个句子的信息，并且当decoder解码第一个词的时候，很明显第一个词与encoder的前几个词相关性最大，所以产生了将encoder所有隐藏层的状态赋予不同的权重作为decoder的一个输入，这就是注意力机制的直觉。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "没有解决一词多义的问题，一个词只有一个固定的词向量。如article具有“物体”和“文章”的意思，但是在word embedding词向量矩阵中只有一个向量表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个双层双向的LSTM模型。每一层的LSTM输出一个隐状态，进行加权平均，再乘以一个尺度缩放系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN必须要先计算前一个节点，把前一个节点处理完的结果传入下一个节点。\n",
    "\n",
    "Transformer可以不用拆分序列一次处理整个序列，提高的执行效率。Transformer对于输入可以并行处理，不像RNN每一时刻输入一个词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer normalization层归一化，batch normalization批归一化。\n",
    "\n",
    "layer normalization是在一个sample里句子长度这个维度进行归一化，batch normalization是在batch这个维度进行归一化，使用层归一化不受句子维度的影响。且由于Transformer是同时将词向量作为输入，所以层归一化是有道理的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer由于是将词向量同时输入进来处理的，丢失了序列之间的前后位置信息，所以需要原始句子里面词和词的位置关系，于是引入位置向量和词向量相加作为输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self-attention: 自注意力机制有q,k,v三个向量，q和k点积运算再乘以一个缩放系数，然后进行softmax，得到的结果作为权重和v相乘得到新的词向量表示，为什么叫自注意力，这个‘自’体现在一句话里不同词语，通过迭代更新wq,wk,wv,矩阵，可以自己学得相似程度高的词之间的联系。而那个典型的seq2seq里面的encoder-decoder的注意力机制是根据decoder序列里面的词和encoder序列里面词的相似程度来判断得到权重。对比一下，seq2seq里面的decoder是注意到了encoder里面的输入词的权重信息，相当于‘外部’注意；而Transformer里面根据输入词向量，迭代更新wq,wk,wv，内部学的，不同词之间的权重信息，相当于‘内部’注意，即自注意力。\n",
    "\n",
    "multi-head attention: 多头自注意力机制就是做多次这样的自注意力机制运算。每次自注意力机制关注词之间不同位置上不同程度的关联，最后平均。multi-head汇总了sefl-attention的所有信息。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masked Multi Self Attention；\n",
    "Layer Norm；\n",
    "Feed Forward；\n",
    "Layer Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先通过无标签的文本去训练生成语言模型，再根据具体的NLP任务（如文本蕴涵、QA、文本分类等），来通过有标签的数据对模型进行fine-tuning。\n",
    "\n",
    " GPT-2 使用参考： https://www.jianshu.com/p/facbf5fca3da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拿周围的词预测中间挖掉的词的注意力模型。基于encoder的双向Transformer采用MLM掩盖部分词，进行无监督训练，采用MLM是为了使得一个词在深层双向训练过程中使用周围的词对mask掉的词进行更好地表征。\n",
    "\n",
    "BERT模型中作者对15%的word token进行mask，而被mask的词，有80%概率真正mask，10%概率替换为其它词，10%概率不变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT的输入是由token embeddings, segmentation embeddings 和position embeddings组合而成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同GPT一样也是迁移学习，在末端根据不同的任务去构造神经网络模型。 \n",
    "\n",
    "单句子分类任务，如情感分析任务，使用[CLS]标签对应的输出，接一个Linear层，然后softmax得到分类结果； 多标签任务，那么用每个词对应的输出相应的做一个分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT：基于decoder的单向Transformer,mask self attention。多层单向的transformer,以decoder为基础单元，而且必须把上一个词的输出作为下一个词的输入，因为使用了decoder，在计算某个词attention时需要把当前词之后词遮盖，别让后续的词语影响输出。\n",
    "\n",
    "BERT:多层双向的transformer,以encoder为基础单元，需要给输入的句子的开头赋予CLS标签，把每个句子用SEP分隔，并使用mask随机以一定的概率替换句子的某些词。\n",
    "    \n",
    "GPT2：和GPT结构一样，只是比GPT的结构更大，训练数据更多，质量更高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
