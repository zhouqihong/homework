{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意力计算公式   \n",
    "$$A = Softmax(Q*K^T/\\sqrt{d})*V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    #计算注意力\n",
    "    def __init__(self,\n",
    "                 attention_dropout=0.0):\n",
    "\n",
    "        super(ScaledDotProductAttention,self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "\n",
    "    def forward(self,q,k,v,scale=None,attn_mask = None):\n",
    "\n",
    "        attention = torch.matmul(q,k.transpose(-2,-1)) # 计算 Q*K^T\n",
    "\n",
    "        if scale:\n",
    "            attention = attention * scale\n",
    "\n",
    "        # mask attention. The attentions between the masked words and\n",
    "        # other words are set to negative infinity\n",
    "        if attn_mask is not None:\n",
    "            attention = attention.masked_fill_(attn_mask,-np.inf) \n",
    "        # 这里掩码会把 Q*K^T里需要被掩盖的部分换成-inf 这样在softmax里该数值就变为零\n",
    "        # 在Encoder里 需要掩盖住填充的0  在Decoder里除了掩盖住填充的0外 还要掩盖住后面的词\n",
    "\n",
    "        attention = self.softmax(attention)\n",
    "        attention = self.dropout(attention)\n",
    "        context = torch.matmul(attention,v)\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多头注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # compute multi heads attention\n",
    "    # 多头注意力的本质是由多个Wq,Wk,Wv计算出多组 Q,K,V从而得到多个向量 \n",
    "    # 这里实现的方式是 由一个大的Wq,Wk,Wv 计算出一组大的Q,K,V 再把这个Q,K,V分成若干个\n",
    "    def __init__(self,\n",
    "                 d_modl=512,\n",
    "                 num_heads=8,\n",
    "                 dropout=0.0):\n",
    "\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "\n",
    "        self.dim_per_head = d_modl // num_heads #计算每个头的维度\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_k = nn.Linear(d_modl, d_modl)\n",
    "        self.linear_v = nn.Linear(d_modl, d_modl)\n",
    "        self.linear_q = nn.Linear(d_modl, d_modl)\n",
    "\n",
    "        self.dot_product_attention = ScaledDotProductAttention(dropout)\n",
    "        self.linear_final = nn.Linear(d_modl,d_modl)\n",
    "        self.norm = nn.LayerNorm(d_modl)\n",
    "\n",
    "\n",
    "    def forward(self, keys, values, queries, attn_mask=None):\n",
    "\n",
    "        residual = queries\n",
    "        batch_size = keys.size(0)\n",
    "        #generate keys,values and queries from inputs\n",
    "        keys = self.linear_k(keys) # 计算Wk * E(输入词向量) = K\n",
    "        values = self.linear_v(values) # Wv * E  = V\n",
    "        queries = self.linear_q(queries) #Wq *E =Q\n",
    "        \n",
    "        #以下做的就是将Q,K,V分别拆分成num_head个 q,k,v\n",
    "        keys = keys.view(batch_size , -1, self.num_heads, self.dim_per_head).transpose(1,2) \n",
    "        values = values.view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1,2)\n",
    "        queries = queries.view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1,2)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.unsqueeze(1).repeat(1,self.num_heads,1,1)\n",
    "\n",
    "        scale = (keys.size(-1)) ** -0.5\n",
    "        #计算注意力\n",
    "        context = self.dot_product_attention(queries,keys,values,scale,attn_mask)\n",
    "        \n",
    "        #将多个头的输出向量拼接合并\n",
    "        context = context.transpose(1,2).contiguous() \\\n",
    "                  .view(batch_size,-1,self.num_heads * self.dim_per_head)\n",
    "\n",
    "        # layer normalization and residual network\n",
    "        return self.norm(residual+self.linear_final(context)) # linear 将拼接够的多头 进行信息融合和映射回d维度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 位置编码   \n",
    "$$PE_{(pos,2i)} = sin(\\frac{pos}{1000^{2i/d_{model}}})$$   \n",
    "$$PE_{(pos,2i+1)} = cos(\\frac{pos}{1000^{2i/d_{model}}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    #compute position encoding\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 max_seq_len,\n",
    "                 dropout=0.0):\n",
    "\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_seq_len,d_model) #初始化位置向量\n",
    "        position = torch.arange(0.,max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0.,d_model,2)*-(math.log(10000.0)/d_model)) #计算分母\n",
    "\n",
    "        pe[:,0::2] = torch.sin(position * div_term) #计算位置编码向量里偶数位子的数值\n",
    "        pe[:,1::2] = torch.cos(position * div_term) #计算位置编码里奇数位置的数值\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\",pe)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = x + Variable(self.pe[:,:x.size(1)],requires_grad=False)\n",
    "\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前向+层归一\n",
    "\n",
    "$$Out = Layernorm(x + W_2*ReLu(W_1+bias)+bias)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalWiseFeedForward(nn.Module):\n",
    "   #前向传播+residual connection\n",
    "    def __init__(self,\n",
    "                 d_model=512,\n",
    "                 ffn_dim=2048,\n",
    "                 dropout=0.0):\n",
    "\n",
    "        super(PositionalWiseFeedForward,self).__init__()\n",
    "\n",
    "        self.w1 = nn.Linear(d_model,ffn_dim)\n",
    "        self.w2 = nn.Linear(ffn_dim,d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        output = self.w2(F.relu(self.w1(x)))\n",
    "        # layer normalization and residual network\n",
    "        return self.norm(x+self.dropout(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model = 512,\n",
    "                 num_heads = 8,\n",
    "                 ffn_dim = 2018,\n",
    "                 dropout = 0.0):\n",
    "\n",
    "        super(EncoderLayer,self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = PositionalWiseFeedForward(d_model, ffn_dim, dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask = None):\n",
    "\n",
    "        context = self.attention(x,x,x,attn_mask)\n",
    "        output = self.feed_forward(context)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 max_seq_len,\n",
    "                 num_layers = 6,\n",
    "                 d_model = 512,\n",
    "                 num_heads = 8,\n",
    "                 ffn_dim = 2048,\n",
    "                 dropout = 0.0):\n",
    "\n",
    "        super(Encoder,self).__init__()\n",
    "        #以下代码是建立num_layer层 \n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "                            [EncoderLayer(d_model,num_heads,ffn_dim,dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.pos_embedding = PositionalEncoding(d_model, max_seq_len,dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seq_embedding):\n",
    "\n",
    "        embedding = seq_embedding(x)\n",
    "        output = self.pos_embedding(embedding)\n",
    "        self_attention_mask = padding_mask(x,x)\n",
    "\n",
    "        for encoder in self.encoder_layers:\n",
    "            output = encoder(output,self_attention_mask)\n",
    "\n",
    "        return self.norm(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 num_heads = 8,\n",
    "                 ffn_dim = 2048,\n",
    "                 dropout = 0.0):\n",
    "\n",
    "        super(DecoderLayer,self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = PositionalWiseFeedForward(d_model, ffn_dim, dropout)\n",
    "\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, self_attn_mask = None,context_attn_mask = None):\n",
    "\n",
    "        dec_ouput  = self.attention(dec_inputs, dec_inputs, dec_inputs ,self_attn_mask)\n",
    "\n",
    "        dec_ouput = self.attention(enc_outputs, enc_outputs,dec_ouput, context_attn_mask)\n",
    "\n",
    "        dec_ouput = self.feed_forward(dec_ouput)\n",
    "\n",
    "        return dec_ouput\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                vocab_size,\n",
    "                 max_seq_len,\n",
    "                 device,\n",
    "                 num_layers = 6,\n",
    "                 d_model  = 512,\n",
    "                 num_heads = 8,\n",
    "                 ffn_dim = 2048,\n",
    "                 dropout = 0.0,\n",
    "                 ):\n",
    "\n",
    "        super(Decoder,self).__init__()\n",
    "        self.device = device\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model,num_heads,ffn_dim,dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.seq_embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_embedding = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.linear = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, enc_output, seq_embedding, context_attn_mask = None):\n",
    "\n",
    "        embedding = seq_embedding(inputs)\n",
    "        output =  embedding + self.pos_embedding(embedding)\n",
    "\n",
    "        self_attention_padding_mask = padding_mask(inputs, inputs)\n",
    "        seq_mask = sequence_mask(inputs).to(self.device)\n",
    "        self_attn_mask = torch.gt((self_attention_padding_mask+seq_mask), 0 )\n",
    "\n",
    "        for decoder in self.decoder_layers:\n",
    "            output = decoder(output, enc_output,self_attn_mask,context_attn_mask)\n",
    "\n",
    "        output = self.linear(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    #Build transformer model\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 max_len,\n",
    "                 device,\n",
    "                 num_layers = 6,\n",
    "                 stack_layers= 6,\n",
    "                 d_model = 512,\n",
    "                 num_heads = 8,\n",
    "                 ffn_dim = 2048,\n",
    "                 dropout = 0.2):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size, max_len, num_layers, d_model, num_heads, ffn_dim, dropout)\n",
    "        self.decoder = Decoder(vocab_size, max_len, num_layers, d_model, num_heads, ffn_dim, dropout)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.linear = nn.Linear(d_model, vocab_size, bias = False)\n",
    "        #self.linear = nn.Softmax(dim = 2)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, src_seq, dec_tgt,dec_in):                           \n",
    "\n",
    "        context_attn_mask_dec = padding_mask(dec_tgt, src_seq)\n",
    "        \n",
    "        en_output = self.encoder(src_seq, self.embedding)\n",
    "        \n",
    "        dec_output = self.decoder(dec_tgt, en_output, self.embedding, context_attn_mask_dec)\n",
    "        \n",
    "        #gen_output = self.generator(image_in, en_output)\n",
    "        \n",
    "        #return dec_output,gen_output\n",
    "        \n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(seq_k, seq_q):\n",
    "\n",
    "    # pad sentence\n",
    "    len_q = seq_q.size(1)\n",
    "    pad_mask = seq_k.eq(0)\n",
    "    pad_mask = pad_mask.unsqueeze(1).expand(-1,len_q,-1)\n",
    "\n",
    "    return pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(txt_in, img_in):\n",
    "    len_txt = txt_in.shape[1]\n",
    "    len_img = img_in.shape[1]\n",
    "    pad_mask = txt_in.eq(0)\n",
    "    len_tot = len_txt + len_img\n",
    "    pad_mask = pad_mask.unsqueeze(1).expand(-1, len_tot, -1)\n",
    "    return pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1,2,3,0,0,0],\n",
    "                       [3,4,0,0,0,0],\n",
    "                       [3,0,0,0,0,0],\n",
    "                       [4,5,6,7,0,0]])\n",
    "img = torch.randn(4,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = padding_mask(inputs,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.zeros(4,11,11)\n",
    "p[:,:,:6] = padding_mask(inputs,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = torch.zeros(4,6,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(seq):\n",
    "\n",
    "    batch_size , seq_len = seq.size()\n",
    "    mask = torch.triu(torch.ones((seq_len, seq_len),dtype = torch.uint8),\n",
    "                      diagonal = 1)\n",
    "    mask = mask.unsqueeze(0).expand(batch_size, -1,-1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False]],\n",
       "\n",
       "        [[False,  True,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False]],\n",
       "\n",
       "        [[False,  True,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False]],\n",
       "\n",
       "        [[False,  True,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_mask(inputs)==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[3,4,5,4],\n",
    "                   [1,2,3,4],\n",
    "                   [5,6,7,8]],\n",
    "                  [[0,0,0,0],\n",
    "                   [9,9,9,9],\n",
    "                   [5,5,5,5]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.zeros(2,2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[3, 4, 5, 4],\n",
       "          [1, 2, 3, 4],\n",
       "          [5, 6, 7, 8]],\n",
       "\n",
       "         [[3, 4, 5, 4],\n",
       "          [1, 2, 3, 4],\n",
       "          [5, 6, 7, 8]]],\n",
       "\n",
       "\n",
       "        [[[0, 0, 0, 0],\n",
       "          [9, 9, 9, 9],\n",
       "          [5, 5, 5, 5]],\n",
       "\n",
       "         [[0, 0, 0, 0],\n",
       "          [9, 9, 9, 9],\n",
       "          [5, 5, 5, 5]]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(1).expand_as(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
